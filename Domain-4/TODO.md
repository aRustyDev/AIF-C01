# Domain 4: Guidelines for Responsible AI

## Explain the development of AI systems that are responsible.

**Objectives:**

- [ ] Identify features of responsible AI (for example, bias, fairness, inclusivity, #task
robustness, safety, veracity).
- [ ] Understand how to use tools to identify features of responsible AI (for #task
example, Guardrails for Amazon Bedrock).
- [ ] Understand responsible practices to select a model (for example, #task
environmental considerations, sustainability).
- [ ] Identify legal risks of working with generative AI (for example, intellectual #task
property infringement claims, biased model outputs, loss of customer trust,
end user risk, hallucinations).
- [ ] Identify characteristics of datasets (for example, inclusivity, diversity, #task
curated data sources, balanced datasets).
- [ ] Understand effects of bias and variance (for example, effects on #task
demographic groups, inaccuracy, overfitting, underfitting).
- [ ] Describe tools to detect and monitor bias, trustworthiness, and truthfulness #task
(for example, analyzing label quality, human audits, subgroup analysis,
Amazon SageMaker Clarify, SageMaker Model Monitor, Amazon Augmented
AI [Amazon A2I]).

## Recognize the importance of transparent and explainable models.

**Objectives:**

- [ ] Understand the differences between models that are transparent and #task
explainable and models that are not transparent and explainable.
- [ ] Understand the tools to identify transparent and explainable models (for #task
example, Amazon SageMaker Model Cards, open source models, data,
licensing).
- [ ] Identify tradeoffs between model safety and transparency (for example, #task
measure interpretability and performance).
- [ ] Understand principles of human-centered design for explainable AI #task

