# Domain 3: Applications of Foundation Models

## Describe design considerations for applications that use foundation models.

**Objectives:**

- [ ] Identify selection criteria to choose pre-trained models (for example, cost, #task
modality, latency, multi-lingual, model size, model complexity,
customization, input/output length).
- [ ] Understand the effect of inference parameters on model responses (for #task
example, temperature, input/output length).
- [ ] Define Retrieval Augmented Generation (RAG) and describe its business #task
applications (for example, Amazon Bedrock, knowledge base).
- [ ] Identify AWS services that help store embeddings within vector databases #task
(for example, Amazon OpenSearch Service, Amazon Aurora, Amazon
Neptune, Amazon DocumentDB [with MongoDB compatibility], Amazon
RDS for PostgreSQL).
- [ ] Explain the cost tradeoffs of various approaches to foundation model #task
customization (for example, pre-training, fine-tuning, in-context learning,
RAG).
- [ ] Understand the role of agents in multi-step tasks (for example, Agents for #task
Amazon Bedrock).

## Choose effective prompt engineering techniques.

**Objectives:**

- [ ] Describe the concepts and constructs of prompt engineering (for example, #task
context, instruction, negative prompts, model latent space).
- [ ] Understand techniques for prompt engineering (for example, chain-of- #task
thought, zero-shot, single-shot, few-shot, prompt templates).
- [ ] Understand the benefits and best practices for prompt engineering (for #task
example, response quality improvement, experimentation, guardrails,
discovery, specificity and concision, using multiple comments).
- [ ] Define potential risks and limitations of prompt engineering (for example, #task
exposure, poisoning, hijacking, jailbreaking).

## Describe the training and fine-tuning process for foundation models.

**Objectives:**

- [ ] Describe the key elements of training a foundation model (for example, #task
pre-training, fine-tuning, continuous pre-training).
- [ ] Define methods for fine-tuning a foundation model (for example, #task
instruction tuning, adapting models for specific domains, transfer learning,
continuous pre-training).
- [ ] Describe how to prepare data to fine-tune a foundation model (for #task
example, data curation, governance, size, labeling, representativeness,
reinforcement learning from human feedback [RLHF]).

## Describe methods to evaluate foundation model performance.

**Objectives:**

- [ ] Understand approaches to evaluate foundation model performance (for #task
example, human evaluation, benchmark datasets).
- [ ] Identify relevant metrics to assess foundation model performance (for #task
example, Recall-Oriented Understudy for Gisting Evaluation [ROUGE],
Bilingual Evaluation Understudy [BLEU], BERTScore).
- [ ] Determine whether a foundation model effectively meets business #task
objectives (for example, productivity, user engagement, task engineering).

